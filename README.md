# Tweet Sentiment Transformers
Sentiment analysis on TweetEval using RoBERTa and DistilBERT

This repository contains code for exploring how large pretrained Transformer models can be adapted to specific downstream tasks. 

The goal is to compare the performance and efficiency of RoBERTa and DistilBERT under two strategies:
- Full fine-tuning (updating all model parameters)
- LoRA fine-tuning (Low-Rank Adaptation)
---

## Repository Structure
```
tweet-sentiment-transformers/
│
├── main.py                 # runs all experiments (both models + both strategies)
├── utils.py                # helper functions: data loading, tokenization, training, metrics
├── requirements.txt        # dependencies
├── .gitignore
├── README.md               # this file
└── results/                # outputs (auto-generated)
```
---

## Dataset
The experiments use the TweetEval Sentiment dataset (cardiffnlp/tweet_eval, 2020), which contains tweets labelled as positive, negative, or neutral.

The dataset is automatically downloaded through the datasets library so no manual download required.

---

## Running experiments

To reproduce all results, execute: `python main.py`

This will:
- Load the TweetEval Sentiment dataset
- Run both RoBERTa and DistilBERT
- Train each under both Full Fine-tuning and LoRA
- Evaluate on the test set and print metrics (Accuracy, F1-score, MCC)

---
## Output

Training outputs, checkpoints, and logs are saved in the `./results/` directory when running the scripts locally. 
These files are not included in the repository to save space, but can be generated by executing `main.py`.

Example output:
```
Running experiments for: cardiffnlp/twitter-roberta-base-sentiment
Full fine-tuning results: ...
LoRA results: ...

Running experiments for: bhadresh-savani/distilbert-base-uncased-emotion
Full fine-tuning results: ...
LoRA results: ...
```

---
## Notes and limitations

- The dataset is automatically cached locally (no data files are included in this repository).
- Pretrained model weights are downloaded from the Hugging Face Hub.
- Training times may vary depending on your GPU/CPU setup.

---
## Reference


---
## Citation
If you use this code or dataset, please cite the TweetEval and LoRA papers listed above.

---
